{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The audio-dataset-converter library (and its dependent libraries) can be used for converting audio datasets from one format into another. It has I/O support for the following domains: Audio classification Speech Please refer to the dataset formats section for more details on supported formats. But the library does not just convert datasets, you can also slot in complex filter pipelines to process/clean the data. On this website you can find examples for: Audio classification Speech Filter usage Docker usage Examples for the additional libraries: Faster whisper Redis Visualization","title":"Home"},{"location":"audio_classification/","text":"Readers and writers for audio classification have the -ac suffix. Plugins # sub-dir to ADAMS # The following converts an audio classification dataset from the sub-dir format (sub-directory names represent the audio classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): adc-convert -l INFO \\ from-subdir-ac \\ -l INFO \\ -i ./subdir/ \\ to-adams-ac \\ -l INFO \\ -o ./adams \\ -c classification sub-dir (randomized train/val/test splits) # By enforcing batch-processing --force_batch and using the randomize-records filter, randomized train/val/test splits (writers typically support generating splits) can be generated like this: adc-convert -l INFO --force_batch \\ from-subdir-ac \\ -l INFO \\ -i ./subdir/ \\ randomize-records \\ -s 42 \\ to-subdir-ac \\ -l INFO \\ -o ./subdir-split \\ -c classification \\ --split_names train val test \\ --split_ratios 70 15 15","title":"Audio classification"},{"location":"audio_classification/#plugins","text":"","title":"Plugins"},{"location":"audio_classification/#sub-dir-to-adams","text":"The following converts an audio classification dataset from the sub-dir format (sub-directory names represent the audio classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): adc-convert -l INFO \\ from-subdir-ac \\ -l INFO \\ -i ./subdir/ \\ to-adams-ac \\ -l INFO \\ -o ./adams \\ -c classification","title":"sub-dir to ADAMS"},{"location":"audio_classification/#sub-dir-randomized-trainvaltest-splits","text":"By enforcing batch-processing --force_batch and using the randomize-records filter, randomized train/val/test splits (writers typically support generating splits) can be generated like this: adc-convert -l INFO --force_batch \\ from-subdir-ac \\ -l INFO \\ -i ./subdir/ \\ randomize-records \\ -s 42 \\ to-subdir-ac \\ -l INFO \\ -o ./subdir-split \\ -c classification \\ --split_names train val test \\ --split_ratios 70 15 15","title":"sub-dir (randomized train/val/test splits)"},{"location":"docker/","text":"Below are examples for using the audio-dataset-converter library via its Docker images . Interactive session # The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/audio-dataset-converter:latest Conversion pipeline # The following converts an audio classification dataset from the sub-dir format (sub-directory names represent the audio classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/audio-dataset-converter:latest \\ adc-convert -l INFO \\ from-subdir-ac \\ -l INFO \\ -i /workspace/input/ \\ to-adams-ac \\ -l INFO \\ -o /workspace/output \\ -c classification NB: The input and output directories are located below the current working directory ( pwd ).","title":"Docker usage"},{"location":"docker/#interactive-session","text":"The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/audio-dataset-converter:latest","title":"Interactive session"},{"location":"docker/#conversion-pipeline","text":"The following converts an audio classification dataset from the sub-dir format (sub-directory names represent the audio classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/audio-dataset-converter:latest \\ adc-convert -l INFO \\ from-subdir-ac \\ -l INFO \\ -i /workspace/input/ \\ to-adams-ac \\ -l INFO \\ -o /workspace/output \\ -c classification NB: The input and output directories are located below the current working directory ( pwd ).","title":"Conversion pipeline"},{"location":"faster_whisper/","text":"Requirements # Requires the audio-dataset-converter-faster-whisper library. Plugins # Transcribing audio # The following commands loads raw audio files (i.e., ones without a transcript) and applies the fw-transcribe filter to generate a transcript using faster-whisper, with the result then being stored in Festvox format: adc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -i \"./raw/*.wav\" \\ -t sp \\ fw-transcribe \\ -l INFO \\ to-festvox-sp \\ -l INFO \\ -o ./festvox Tools # Generate SRT subtitles # The adc-srt allows generating subtitles in SRT format from audio and video files. The following example generates subtitle files for all .mp4 file, alongside the video files: adc-srt \\ -l INFO \\ -i ./input/*.mp4 In this example, the .srt files generated from .wav files get placed in a separate output directory: adc-srt \\ -l INFO \\ -i ./input/*.wav \\ -o ./output","title":"Faster whisper"},{"location":"faster_whisper/#requirements","text":"Requires the audio-dataset-converter-faster-whisper library.","title":"Requirements"},{"location":"faster_whisper/#plugins","text":"","title":"Plugins"},{"location":"faster_whisper/#transcribing-audio","text":"The following commands loads raw audio files (i.e., ones without a transcript) and applies the fw-transcribe filter to generate a transcript using faster-whisper, with the result then being stored in Festvox format: adc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -i \"./raw/*.wav\" \\ -t sp \\ fw-transcribe \\ -l INFO \\ to-festvox-sp \\ -l INFO \\ -o ./festvox","title":"Transcribing audio"},{"location":"faster_whisper/#tools","text":"","title":"Tools"},{"location":"faster_whisper/#generate-srt-subtitles","text":"The adc-srt allows generating subtitles in SRT format from audio and video files. The following example generates subtitle files for all .mp4 file, alongside the video files: adc-srt \\ -l INFO \\ -i ./input/*.mp4 In this example, the .srt files generated from .wav files get placed in a separate output directory: adc-srt \\ -l INFO \\ -i ./input/*.wav \\ -o ./output","title":"Generate SRT subtitles"},{"location":"filters/","text":"The following sections only show snippets of commands, as there are quite a number of filters available. Annotation management # strip-annotations - removes all annotations Audio management # convert-to-mono - ensures that audio data is mono convert-to-wav - ensures that audio data is in WAV format trim-silence - for removing chunks of silence Augmentation # pitch-shift - for shifting the pitch of audio samples time-stretch - for speeding up/slowing down samples Meta-data management # metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the audio file name via a regular expression split - adds the field split to the meta-data of the record passing through, which can be acted on with other filters (or stored in the output) Record management # A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-negatives - removes records from the stream that have no annotations max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of audio files, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream","title":"Filter usage"},{"location":"filters/#annotation-management","text":"strip-annotations - removes all annotations","title":"Annotation management"},{"location":"filters/#audio-management","text":"convert-to-mono - ensures that audio data is mono convert-to-wav - ensures that audio data is in WAV format trim-silence - for removing chunks of silence","title":"Audio management"},{"location":"filters/#augmentation","text":"pitch-shift - for shifting the pitch of audio samples time-stretch - for speeding up/slowing down samples","title":"Augmentation"},{"location":"filters/#meta-data-management","text":"metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the audio file name via a regular expression split - adds the field split to the meta-data of the record passing through, which can be acted on with other filters (or stored in the output)","title":"Meta-data management"},{"location":"filters/#record-management","text":"A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-negatives - removes records from the stream that have no annotations max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of audio files, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream","title":"Record management"},{"location":"multi/","text":"Most of the time, conversion pipelines will only need to read from one source and output to a single target. However, there can be cases where datasets of different types need merging ( multiple inputs ) or datasets of different types need to generated for different frameworks ( multiple outputs ). To cater for these scenarios, the following two meta plugins are available: from-multi - reads from one or more sources using the specified readers to-multi - forwards the incoming data to one or more writers There is one restriction, each of the base reader/writer must be from the same data domain. Multiple inputs # The following command reads a dataset in Festvox and ADAMS format, with the combined output being saved in CommonVoice format: adc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -t sp \\ -r \"from-festvox-sp -l INFO -i {CWD}/input/*.txt\" \\ \"from-adams-sp -l INFO -i {CWD}/input/*.report -t transcript\" \\ to-commonvoice-sp \\ -l INFO \\ -o \"{CWD}/output\" Multiple outputs # Below, the source data is in ADAMS speech format and will be converted to Festvox and CommonVoice format: adc-convert \\ -l INFO \\ from-adams-sp \\ -l INFO \\ -i \"{CWD}/input/*.report\" \\ to-multi \\ -l INFO \\ -t sp \\ -w \"to-festvox-sp -l INFO -o {CWD}/output/festvox\" \\ \"to-commonvoice-sp -l INFO -o {CWD}/output/commonvoice\" Sub-pipelines # With the tee meta-filter, it is possible to filter the audio files coming through with a separate sub-pipeline. E.g., converting the incoming data into multiple output formats with their own preprocessing. The following command loads the Festvox speech data and saves them in ADAMS and split ADAMS format (after trimming silences) in one command: adc-convert \\ -l INFO \\ from-festvox-sp \\ -l INFO \\ -i \"./festvox/*.txt\" \\ tee \\ -f \"to-adams-sp -o ./adams-tee/ -t transcript\" \\ tee \\ -f \"trim-silence to-adams-sp -o ./adams-split-tee/ -t transcript --split_names train val test --split_ratios 70 15 15\"","title":"Multiple I/O"},{"location":"multi/#multiple-inputs","text":"The following command reads a dataset in Festvox and ADAMS format, with the combined output being saved in CommonVoice format: adc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -t sp \\ -r \"from-festvox-sp -l INFO -i {CWD}/input/*.txt\" \\ \"from-adams-sp -l INFO -i {CWD}/input/*.report -t transcript\" \\ to-commonvoice-sp \\ -l INFO \\ -o \"{CWD}/output\"","title":"Multiple inputs"},{"location":"multi/#multiple-outputs","text":"Below, the source data is in ADAMS speech format and will be converted to Festvox and CommonVoice format: adc-convert \\ -l INFO \\ from-adams-sp \\ -l INFO \\ -i \"{CWD}/input/*.report\" \\ to-multi \\ -l INFO \\ -t sp \\ -w \"to-festvox-sp -l INFO -o {CWD}/output/festvox\" \\ \"to-commonvoice-sp -l INFO -o {CWD}/output/commonvoice\"","title":"Multiple outputs"},{"location":"multi/#sub-pipelines","text":"With the tee meta-filter, it is possible to filter the audio files coming through with a separate sub-pipeline. E.g., converting the incoming data into multiple output formats with their own preprocessing. The following command loads the Festvox speech data and saves them in ADAMS and split ADAMS format (after trimming silences) in one command: adc-convert \\ -l INFO \\ from-festvox-sp \\ -l INFO \\ -i \"./festvox/*.txt\" \\ tee \\ -f \"to-adams-sp -o ./adams-tee/ -t transcript\" \\ tee \\ -f \"trim-silence to-adams-sp -o ./adams-split-tee/ -t transcript --split_names train val test --split_ratios 70 15 15\"","title":"Sub-pipelines"},{"location":"pyfunc/","text":"No library can dream of offering all the required functionality. Especially for one-off tasks, it makes no sense to develop a whole new plugin library. Hence, there are the following generic plugins that allow the user to utilize custom Python functions: reader: from-pyfunc - takes a single string as input and outputs an iterable of audio containers (as per specified data type) filter: pyfunc-filter - takes a single audio container or an iterable of them as input and outputs a single container or an iterable of them (as per specified input and output data types) writer: to-pyfunc - processes a single audio container or an iterable of them as per specified data type and an optional split name In order to use such a custom function, they must be specified in the following format (option: -f/--function ): module_name:function_name If the code below were available through module my.code , then the function specifications would be as follows: reader: my.code:pyfunc_reader filter: my.code:pyfunc_filter writer: my.code:pyfunc_writer from typing import Iterable from adc.api import AudioClassificationData, make_list, flatten_list # reader: generates audio classification containers from the path def pyfunc_reader(path: str) -> Iterable[AudioClassificationData]: return [AudioClassificationData(source=path)] # filter: simply adds a note to the meta-data def pyfunc_filter(data): result = [] for item in make_list(data): if not item.has_metadata(): meta = dict() else: meta = item.get_metadata() meta[\"note\"] = \"filtered by a python function!\" item.set_metadata(meta) result.append(item) return flatten_list(result) # writer: simply outputs name and meta-data and, if present, also the split def pyfunc_writer(data: AudioClassificationData, split: str = None): if split is None: print(\"name: \", data.audio_name, \", meta:\", data.get_metadata()) else: print(\"split:\", split, \", name:\", data.audio_name, \", meta:\", data.get_metadata())","title":"External functions"},{"location":"redis/","text":"Requirements # Requires the audio-dataset-converter-redis library. Plugins # Transcribing audio # The following commands loads raw audio files (i.e., ones without a transcript) and applies the redis-transcribe filter to generate a transcript using faster-whisper, with the result then being stored in Festvox format: First, start the faster-whisper process via Docker (using the CPU): docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --shm-size 8G \\ --net=host \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -it public.aml-repo.cms.waikato.ac.nz:443/pytorch/python-faster-whisper:1.0.2_cpu \\ fw_predict_redis \\ --redis_in audio \\ --redis_out transcript \\ --model_size base \\ --verbose And now apply the pipeline to have the audio files transcribed: adc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -i \"./raw/*.wav\" \\ -t sp \\ redis-transcribe \\ -l INFO \\ --channel_out audio \\ --channel_in transcript \\ to-festvox-sp \\ -l INFO \\ -o ./festvox","title":"Redis"},{"location":"redis/#requirements","text":"Requires the audio-dataset-converter-redis library.","title":"Requirements"},{"location":"redis/#plugins","text":"","title":"Plugins"},{"location":"redis/#transcribing-audio","text":"The following commands loads raw audio files (i.e., ones without a transcript) and applies the redis-transcribe filter to generate a transcript using faster-whisper, with the result then being stored in Festvox format: First, start the faster-whisper process via Docker (using the CPU): docker run --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ --shm-size 8G \\ --net=host \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -it public.aml-repo.cms.waikato.ac.nz:443/pytorch/python-faster-whisper:1.0.2_cpu \\ fw_predict_redis \\ --redis_in audio \\ --redis_out transcript \\ --model_size base \\ --verbose And now apply the pipeline to have the audio files transcribed: adc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -i \"./raw/*.wav\" \\ -t sp \\ redis-transcribe \\ -l INFO \\ --channel_out audio \\ --channel_in transcript \\ to-festvox-sp \\ -l INFO \\ -o ./festvox","title":"Transcribing audio"},{"location":"speech/","text":"Readers and writers for speech have the -sp suffix. Download the Iris Living Audio Dataset dataset in Festvox format and extract it. Plugins # Festvox to ADAMS # The following converts the Festvoc dataset into ADAMS format, storing the transcript in the transcript field: adc-convert \\ -l INFO \\ from-festvox-sp \\ -l INFO \\ -i \"./festvox/*.txt\" \\ to-adams-sp \\ -l INFO \\ -o ./adams \\ -t transcript Festvox to ADAMS (train/val/test splits) # You can also split the data, e.g., into train, validation and test subsets. The following converts the Festvox into ADAMS format: adc-convert \\ -l INFO \\ from-festvox-sp \\ -l INFO \\ -i \"./festvox/*.txt\" \\ to-adams-sp \\ -l INFO \\ -o ./adams-split \\ -t transcript \\ --split_names train val test \\ --split_ratios 70 15 15 NB: The subsets will be placed into sub-directories according to the split name.","title":"Speech"},{"location":"speech/#plugins","text":"","title":"Plugins"},{"location":"speech/#festvox-to-adams","text":"The following converts the Festvoc dataset into ADAMS format, storing the transcript in the transcript field: adc-convert \\ -l INFO \\ from-festvox-sp \\ -l INFO \\ -i \"./festvox/*.txt\" \\ to-adams-sp \\ -l INFO \\ -o ./adams \\ -t transcript","title":"Festvox to ADAMS"},{"location":"speech/#festvox-to-adams-trainvaltest-splits","text":"You can also split the data, e.g., into train, validation and test subsets. The following converts the Festvox into ADAMS format: adc-convert \\ -l INFO \\ from-festvox-sp \\ -l INFO \\ -i \"./festvox/*.txt\" \\ to-adams-sp \\ -l INFO \\ -o ./adams-split \\ -t transcript \\ --split_names train val test \\ --split_ratios 70 15 15 NB: The subsets will be placed into sub-directories according to the split name.","title":"Festvox to ADAMS (train/val/test splits)"},{"location":"visualization/","text":"Requirements # Requires the audio-dataset-converter-visualization library. Plugins # For the following examples, data from the LJ Speech Dataset was used. Mel spectrogram # to-mel-spectrogram - outputs Mel spectrogram images adc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -i \"./input/*.wav\" \\ -t sp \\ to-mel-spectrogram \\ -l INFO \\ -o ./output MFCC spectrogram # to-mfcc-spectrogram - outputs Mel-frequency cepstral coefficients images adc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -i \"./input/*.wav\" \\ -t sp \\ to-mfcc-spectrogram \\ -l INFO \\ -o ./output STFT spectrogram # to-stft-spectrogram - outputs short time fourier transform (STFT) spectrogram images adc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -i \"./input/*.wav\" \\ -t sp \\ to-stft-spectrogram \\ -l INFO \\ -o ./output","title":"Visualization"},{"location":"visualization/#requirements","text":"Requires the audio-dataset-converter-visualization library.","title":"Requirements"},{"location":"visualization/#plugins","text":"For the following examples, data from the LJ Speech Dataset was used.","title":"Plugins"},{"location":"visualization/#mel-spectrogram","text":"to-mel-spectrogram - outputs Mel spectrogram images adc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -i \"./input/*.wav\" \\ -t sp \\ to-mel-spectrogram \\ -l INFO \\ -o ./output","title":"Mel spectrogram"},{"location":"visualization/#mfcc-spectrogram","text":"to-mfcc-spectrogram - outputs Mel-frequency cepstral coefficients images adc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -i \"./input/*.wav\" \\ -t sp \\ to-mfcc-spectrogram \\ -l INFO \\ -o ./output","title":"MFCC spectrogram"},{"location":"visualization/#stft-spectrogram","text":"to-stft-spectrogram - outputs short time fourier transform (STFT) spectrogram images adc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -i \"./input/*.wav\" \\ -t sp \\ to-stft-spectrogram \\ -l INFO \\ -o ./output","title":"STFT spectrogram"}]}